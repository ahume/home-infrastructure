kube-prometheus-stack:
  defaultRules:
    create: false

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true
    adminPassword: zpn8xuv-rta0HXG9bwg # Grafana is not exposed outside network.
    env:
      GF_INSTALL_PLUGINS: flant-statusmap-panel,ae3e-plotly-panel

    ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
    ##
    forceDeployDatasources: false

    ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
    ##
    forceDeployDashboards: false

    ## Deploy default dashboards
    ##
    defaultDashboardsEnabled: false

    ## Timezone for the default dashboards
    ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
    ##
    defaultDashboardsTimezone: utc

    # Defined in secret-values.yaml
    # adminPassword: prom-operator

    rbac:
      ## If true, Grafana PSPs will be created
      ##
      pspEnabled: false

    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
      labels: {}
      hosts:
        - grafana.hunet.uk
      path: /
      tls:
      - secretName: tls-cert-grafana.hunet.uk
        hosts:
        - grafana.hunet.uk


  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false


  prometheus:
    ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
    labels: {}
    hosts:
      - prometheus.hunet.uk
    paths:
      - /
    tls:
      - secretName: tls-cert-prometheus.hunet.uk
        hosts:
          - prometheus.hunet.uk

    prometheusSpec:
      ## External URL at which Prometheus will be reachable.
      ##
      externalUrl: "https://prometheus.hunet.uk"

      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the servicemonitors created
      ##
      serviceMonitorSelectorNilUsesHelmValues: false

      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the podmonitors created
      ##
      podMonitorSelectorNilUsesHelmValues: false

      ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the probes created
      ##
      probeSelectorNilUsesHelmValues: false

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storageSpec:
      ## Using PersistentVolumeClaim
      ##
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi
          # selector: {}

      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs:
        - job_name: external-node-exporter
          static_configs:
            - targets:
              - 10.10.10.10:9100
              - 10.10.10.11:9100
        - job_name: rpitemp
          static_configs:
          - targets: ['192.168.10.236:7028']

