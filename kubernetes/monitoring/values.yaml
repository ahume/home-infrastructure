kube-prometheus-stack:
  defaultRules:
    create: false

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true
    adminPassword: zpn8xuv-rta0HXG9bwg # Grafana is not exposed outside network.
    env:
      GF_INSTALL_PLUGINS: flant-statusmap-panel,ae3e-plotly-panel

    ## We don'deploy the default dashboards as there's loads of them, and we aren't going to
    ## do anything with 99% of them. We create our own custom ones.
    ##
    defaultDashboardsEnabled: false

    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
      labels: {}
      hosts:
        - grafana.hunet.uk
      path: /
      tls:
      - secretName: tls-cert-grafana.hunet.uk
        hosts:
        - grafana.hunet.uk

    resources:
      requests:
        cpu: 0.02
        memory: 100Mi
      limits:
        cpu: 0.02
        memory: 100Mi


  ## Our k3s is doesn't have an etcd to scrape.
  ##
  kubeEtcd:
    enabled: false

  prometheus:
    ingress:
      enabled: true
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
      labels: {}
      hosts:
        - prometheus.hunet.uk
      paths:
        - /
      tls:
        - secretName: tls-cert-prometheus.hunet.uk
          hosts:
            - prometheus.hunet.uk

    prometheusSpec:
      ## External URL at which Prometheus will be reachable.
      ##
      externalUrl: "https://prometheus.hunet.uk"

      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the servicemonitors created
      ##
      serviceMonitorSelectorNilUsesHelmValues: false

      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the podmonitors created
      ##
      podMonitorSelectorNilUsesHelmValues: false

      ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the probes created
      ##
      probeSelectorNilUsesHelmValues: false

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storageSpec:
      ## Using PersistentVolumeClaim
      ##
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi
          # selector: {}

      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs:
        - job_name: external-node-exporter
          static_configs:
            - targets:
              - 10.10.10.10:9100
              - 10.10.10.11:9100
        - job_name: rpitemp
          static_configs:
          - targets: ['192.168.10.236:7028']

